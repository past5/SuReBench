{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96581c7c-ddde-46da-b8c9-0c21ab20ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Hugging Face datasets library\n",
    "!pip install datasets\n",
    "!pip install deepeval\n",
    "!pip install replicate\n",
    "!pip install openai\n",
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc04912-0b66-41b0-bd79-24c1eddb2e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12969c10-ff64-488a-9f6a-f3bcef4d83c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of sampled records: {len(sampled_data)}\")\n",
    "\n",
    "print(sampled_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bebecb-1c7e-475a-a355-b09b588bb195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Set the OpenAI API key\n",
    "client = OpenAI(api_key='sk-xxx')\n",
    "\n",
    "def generate_summary(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Provide a summary of the following text without any introductory or concluding remarks:\\n\\n{text}\"}],\n",
    "        max_tokens=512,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    output = response.choices[0].message.content.strip()\n",
    "    return output\n",
    "\n",
    "gpt_summaries = []\n",
    "\n",
    "for i, record in enumerate(sampled_data):\n",
    "    summary = generate_summary(record['article'])\n",
    "    if summary:\n",
    "        if isinstance(summary, list):\n",
    "            appended_summary = ''.join(summary).strip()\n",
    "        else:\n",
    "            appended_summary = summary.strip()\n",
    "        gpt_summaries.append(appended_summary)\n",
    "    else:\n",
    "        gpt_summaries.append(\"Error\")\n",
    "\n",
    "    # Add sleep to limit requests to 500 per minute (1 request per 0.12 seconds)\n",
    "    time.sleep(0.12)\n",
    "\n",
    "    # Print progress\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"{i + 1} summaries generated\")\n",
    "\n",
    "for i, summary in enumerate(gpt_summaries[:5]):\n",
    "    print(f\"Summary {i+1}: {summary}\")\n",
    "\n",
    "print(f\"Number of summaries generated: {len(gpt_summaries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a2fd3-9218-4fd0-8388-9c1955711c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "import os\n",
    "import time\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_BRTJfgOH9kBMwDYPEISLQdaWT6wMSWC0DDCLV\"\n",
    "model = replicate.models.get(\"meta/meta-llama-3-70b-instruct\")\n",
    "\n",
    "def generate_summary(text):\n",
    "    output = replicate.run(\n",
    "        \"meta/meta-llama-3-70b-instruct\",\n",
    "        input={\"max_tokens\": 512, \"prompt\": f\"Provide a summary of the following text without any introductory or concluding remarks: {text}\", \"temperature\": 0.7}\n",
    "    )\n",
    "    return output\n",
    "\n",
    "llama_summaries = []\n",
    "\n",
    "for i, record in enumerate(sampled_data):\n",
    "    summary = generate_summary(record['article'])\n",
    "    if summary:\n",
    "        if isinstance(summary, list):\n",
    "            appended_summary = ''.join(summary).strip()\n",
    "        else:\n",
    "            appended_summary = summary.strip()\n",
    "        llama_summaries.append(appended_summary)\n",
    "    else:\n",
    "        llama_summaries.append(\"Error\")\n",
    "\n",
    "    # Add sleep to limit requests to 600 per minute (1 request per 0.1 seconds)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    # Print progress\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"{i + 1} summaries generated\")\n",
    "\n",
    "for i, summary in enumerate(llama_summaries[:5]):\n",
    "    print(f\"Summary {i+1}: {summary}\")\n",
    "\n",
    "print(f\"Number of summaries generated: {len(llama_summaries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c07383-5c62-48dd-b1a5-0054db100de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Ensure the length of generated summaries matches the sampled data\n",
    "assert len(llama_summaries) == len(sampled_data), \"Mismatch between summaries and sampled data.\"\n",
    "\n",
    "# Create data records\n",
    "data_records = []\n",
    "for i, record in enumerate(sampled_data):\n",
    "    data_records.append({\n",
    "        \"id\": record['id'],\n",
    "        \"article\": record['article'],\n",
    "        \"generated_summary_orig_llama3-70b-intruct\": llama_summaries[i],\n",
    "        \"generated_summary_orig_gpt4o\": gpt_summaries[i]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data_records)\n",
    "\n",
    "df.to_csv(\"dailymail_generated_summaries.csv\", index=False)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051eb29-9429-482f-a28e-4108f16bbf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import ToxicityMetric\n",
    "from deepeval import evaluate\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import SummarizationMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd52046-9ed3-4eef-a5c0-8965cc5fcc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxicity_score_generator(data:pd.DataFrame, target_col:str)-> pd.DataFrame:\n",
    "  scores = []\n",
    "  for index, row in data.iterrows():\n",
    "    metric = ToxicityMetric(threshold=0)\n",
    "    test_case = LLMTestCase(\n",
    "    input=\"How toxic is this passage?\",\n",
    "    actual_output=row[target_col]\n",
    ")\n",
    "    metric.measure(test_case)\n",
    "    scores.append({\n",
    "        \"id\": row['id'],\n",
    "        'toxicity_score': metric.score,\n",
    "        'toxicity_justification': metric.reason\n",
    "    }\n",
    "    )\n",
    "  tox_scores = pd.DataFrame(scores)\n",
    "\n",
    "  return tox_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23880dd-0c5e-4e47-a4a3-f6d76eb92456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_score_generator(data: pd.DataFrame, article_col: str, summary_col: str) -> pd.DataFrame:\n",
    "    scores = []\n",
    "    for index, row in data.iterrows():\n",
    "        metric = SummarizationMetric(\n",
    "            threshold=0,\n",
    "            model=\"gpt-4\",\n",
    "            include_reason=True\n",
    "        )\n",
    "        test_case = LLMTestCase(\n",
    "            input=row[article_col],\n",
    "            actual_output=row[summary_col]\n",
    "        )\n",
    "        metric.measure(test_case)\n",
    "        scores.append({\n",
    "            \"id\": row['id'],\n",
    "            'summary_score': metric.score,\n",
    "            'summary_justification': metric.reason\n",
    "        })\n",
    "    summary_scores = pd.DataFrame(scores)\n",
    "    return summary_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f70f38-5a94-4733-b3ee-a6f3f47018b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tox_orig = toxicity_score_generator(df, 'article')\n",
    "tox_llama = toxicity_score_generator(df, 'generated_summary_orig_llama3-70b-intruct')\n",
    "tox_gpt = toxicity_score_generator(df, 'generated_summary_orig_gpt4o')\n",
    "sum_llama = summary_score_generator(df, 'article', 'generated_summary_orig_llama3-70b-intruct')\n",
    "sum_gpt = summary_score_generator(df, 'article', 'generated_summary_orig_gpt4o')\n",
    "\n",
    "# Merge all the scores\n",
    "combined = tox_orig.merge(tox_llama, on='id', suffixes=('_orig', '_llama'))\n",
    "combined = combined.merge(tox_gpt, on='id', suffixes=('', '_gpt'))\n",
    "combined = combined.merge(sum_llama, on='id', suffixes=('', '_sum_llama'))\n",
    "combined = combined.merge(sum_gpt, on='id', suffixes=('', '_sum_gpt'))\n",
    "\n",
    "combined = combined.rename(columns={\n",
    "    'toxicity_score': 'toxicity_score_orig',\n",
    "    'toxicity_justification': 'toxicity_justification_orig',\n",
    "    'toxicity_score_llama': 'toxicity_score_llama',\n",
    "    'toxicity_justification_llama': 'toxicity_justification_llama',\n",
    "    'toxicity_score_gpt': 'toxicity_score_gpt',\n",
    "    'toxicity_justification_gpt': 'toxicity_justification_gpt',\n",
    "    'summary_score': 'summary_score_llama',\n",
    "    'summary_justification': 'summary_justification_llama',\n",
    "    'summary_score_sum_gpt': 'summary_score_gpt',\n",
    "    'summary_justification_sum_gpt': 'summary_justification_gpt'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f05c3-bcc1-4354-bf6a-9858a2d4cd43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf59ad88-38d6-41da-909d-8f4b375c2cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
